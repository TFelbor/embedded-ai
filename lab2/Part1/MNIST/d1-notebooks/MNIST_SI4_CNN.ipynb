{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST SI4 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and format MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test  = x_test.astype('float32')  / 255\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)) # 'channels_last' format\n",
    "x_test  = x_test.reshape((10000, 28, 28, 1)) # 'channels_last' format\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test,  10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save validation data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('x_test.csv', x_test.reshape((x_test.shape[0], -1))[0:250], delimiter=',', fmt='%s') \n",
    "np.savetxt('y_test.csv', y_test[0:250], delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Improvements Explained\n",
    "\n",
    "### What Changed and Why It Matters\n",
    "\n",
    "#### 1. **Increased Convolutional Filters (2 → 32, then 64)**\n",
    "- **Original**: 2 filters\n",
    "- **Improved**: 32 filters in first layer, 64 in second layer\n",
    "\n",
    "**Why this helps:**\n",
    "- Each filter learns to detect different features (edges, curves, corners, patterns)\n",
    "- With only 2 filters, the model could only learn 2 different features total\n",
    "- 32 filters allows learning 32 diverse patterns simultaneously (e.g., horizontal lines, diagonal lines, corners, circles)\n",
    "- More filters = richer representation of what makes each digit unique\n",
    "- *Analogy*: Like having more \"detectives\" looking for different clues to identify digits\n",
    "\n",
    "#### 2. **Multiple Convolutional Layers (1 → 2)**\n",
    "- **Original**: Single Conv2D layer\n",
    "- **Improved**: Two stacked Conv2D layers\n",
    "\n",
    "**Why this helps:**\n",
    "- First layer learns low-level features (simple edges, lines)\n",
    "- Second layer learns high-level features by combining outputs from first layer (shapes, loops, structures)\n",
    "- This hierarchical learning is more efficient and biologically inspired (like human visual cortex)\n",
    "- Stacking layers creates \"feature pyramids\" - detecting increasingly complex patterns\n",
    "- *Analogy*: First layer = learning individual brush strokes; Second layer = recognizing full shapes\n",
    "\n",
    "#### 3. **MaxPooling Layers (0 → 2)**\n",
    "- **Original**: No pooling despite importing it\n",
    "- **Improved**: MaxPool2D after each Conv layer\n",
    "\n",
    "**Why this helps:**\n",
    "- Reduces spatial dimensions (28×28 → 14×14 → 7×7)\n",
    "- Keeps only the \"most important\" values in each region (takes the maximum)\n",
    "- **Robustness**: Model becomes invariant to small shifts/rotations of digits\n",
    "- **Efficiency**: Fewer parameters reduces overfitting risk\n",
    "- **Speed**: Smaller feature maps = faster computation\n",
    "- *Example*: If a \"7\" is shifted 1-2 pixels, MaxPool helps the model still recognize it\n",
    "\n",
    "#### 4. **Larger Dense Layers (10 → 128 → 64 → 10)**\n",
    "- **Original**: Single dense layer with 10 units (one per digit class)\n",
    "- **Improved**: Two hidden dense layers (128 and 64 units) then output layer\n",
    "\n",
    "**Why this helps:**\n",
    "- Learns non-linear decision boundaries in feature space\n",
    "- First dense layer (128 units) = complex classifier combining all features\n",
    "- Second dense layer (64 units) = further refining the classification\n",
    "- More parameters in hidden layers allow learning more complex patterns\n",
    "- *Analogy*: Like having multiple \"expert committees\" voting on what digit it is, rather than one quick decision\n",
    "\n",
    "#### 5. **More Training Epochs (3 → 15)**\n",
    "- **Original**: 3 epochs (only 3 passes through entire dataset)\n",
    "- **Improved**: 15 epochs\n",
    "\n",
    "**Why this helps:**\n",
    "- More opportunities to update weights and reduce loss\n",
    "- Model converges better with a more powerful architecture\n",
    "- Can observe if validation accuracy plateaus (overfitting indicator)\n",
    "- *Trade-off*: More epochs = longer training, but within reason it improves generalization\n",
    "\n",
    "#### 6. **Batch Size Optimization (32 → 128)**\n",
    "- **Original**: Default batch size (32)\n",
    "- **Improved**: Batch size of 128\n",
    "\n",
    "**Why this helps:**\n",
    "- Larger batches = more stable gradient estimates\n",
    "- Faster training with better GPU utilization\n",
    "- Better generalization (averaging gradients over more samples)\n",
    "\n",
    "### Summary: How These Work Together\n",
    "\n",
    "```\n",
    "Input (28×28×1) \n",
    "    ↓\n",
    "Conv2D (32 filters) → learns basic features (edges, corners)\n",
    "    ↓\n",
    "MaxPool (2×2) → reduces noise, keeps important features\n",
    "    ↓\n",
    "Conv2D (64 filters) → learns complex patterns (digit shapes, structures)\n",
    "    ↓\n",
    "MaxPool (2×2) → further noise reduction\n",
    "    ↓\n",
    "Flatten → converts 2D features to 1D vector\n",
    "    ↓\n",
    "Dense (128 units) → complex decision boundary\n",
    "    ↓\n",
    "Dense (64 units) → refined classification\n",
    "    ↓\n",
    "Dense (10 units, softmax) → probability for each digit (0-9)\n",
    "```\n",
    "\n",
    "**Expected Impact:**\n",
    "- **Original model**: ~95-97% accuracy (limited feature learning)\n",
    "- **Improved model**: ~98-99% accuracy (hierarchical feature learning + regularization)\n",
    "\n",
    "The key principle: **Deep learning excels when you give it multiple layers to progressively extract higher-level features from raw input.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">232,650</span> (908.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m232,650\u001b[0m (908.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">232,650</span> (908.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m232,650\u001b[0m (908.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28, 1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 27ms/step - categorical_accuracy: 0.8461 - loss: 0.5352 - val_categorical_accuracy: 0.9813 - val_loss: 0.0553\n",
      "Epoch 2/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - categorical_accuracy: 0.9803 - loss: 0.0630 - val_categorical_accuracy: 0.9846 - val_loss: 0.0463\n",
      "Epoch 3/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - categorical_accuracy: 0.9865 - loss: 0.0419 - val_categorical_accuracy: 0.9859 - val_loss: 0.0437\n",
      "Epoch 4/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - categorical_accuracy: 0.9912 - loss: 0.0291 - val_categorical_accuracy: 0.9861 - val_loss: 0.0422\n",
      "Epoch 5/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - categorical_accuracy: 0.9919 - loss: 0.0255 - val_categorical_accuracy: 0.9915 - val_loss: 0.0252\n",
      "Epoch 6/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - categorical_accuracy: 0.9941 - loss: 0.0186 - val_categorical_accuracy: 0.9902 - val_loss: 0.0335\n",
      "Epoch 7/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - categorical_accuracy: 0.9941 - loss: 0.0186 - val_categorical_accuracy: 0.9875 - val_loss: 0.0405\n",
      "Epoch 8/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - categorical_accuracy: 0.9962 - loss: 0.0120 - val_categorical_accuracy: 0.9883 - val_loss: 0.0376\n",
      "Epoch 9/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - categorical_accuracy: 0.9966 - loss: 0.0102 - val_categorical_accuracy: 0.9907 - val_loss: 0.0311\n",
      "Epoch 10/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - categorical_accuracy: 0.9968 - loss: 0.0094 - val_categorical_accuracy: 0.9913 - val_loss: 0.0325\n",
      "Epoch 11/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - categorical_accuracy: 0.9969 - loss: 0.0088 - val_categorical_accuracy: 0.9925 - val_loss: 0.0290\n",
      "Epoch 12/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 27ms/step - categorical_accuracy: 0.9978 - loss: 0.0064 - val_categorical_accuracy: 0.9918 - val_loss: 0.0308\n",
      "Epoch 13/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 33ms/step - categorical_accuracy: 0.9980 - loss: 0.0059 - val_categorical_accuracy: 0.9910 - val_loss: 0.0375\n",
      "Epoch 14/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - categorical_accuracy: 0.9984 - loss: 0.0048 - val_categorical_accuracy: 0.9921 - val_loss: 0.0330\n",
      "Epoch 15/15\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 27ms/step - categorical_accuracy: 0.9984 - loss: 0.0052 - val_categorical_accuracy: 0.9925 - val_loss: 0.0325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1699ae310>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - 3ms/step - categorical_accuracy: 0.9925 - loss: 0.0325\n",
      "313/313 - 1s - 3ms/step - categorical_accuracy: 0.9925 - loss: 0.0325\n",
      "313/313 - 1s - 3ms/step - categorical_accuracy: 0.9925 - loss: 0.0325\n",
      "Validation Accuracy: 0.9925\n",
      "\u001b[1m  1/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 44ms/stepValidation Accuracy: 0.9925\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "tf.Tensor(\n",
      "[[ 976    1    0    0    0    0    0    2    1    0]\n",
      " [   0 1133    0    0    0    0    1    1    0    0]\n",
      " [   0    0 1027    0    0    0    1    2    2    0]\n",
      " [   1    0    4  998    0    2    0    2    3    0]\n",
      " [   0    0    0    0  977    0    2    0    0    3]\n",
      " [   0    0    1    7    0  880    3    1    0    0]\n",
      " [   4    2    0    0    1    1  948    0    2    0]\n",
      " [   0    1    2    0    1    0    0 1022    1    1]\n",
      " [   2    0    1    1    0    0    0    2  967    1]\n",
      " [   0    0    0    0    6    1    0    4    1  997]], shape=(10, 10), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 976    1    0    0    0    0    0    2    1    0]\n",
      " [   0 1133    0    0    0    0    1    1    0    0]\n",
      " [   0    0 1027    0    0    0    1    2    2    0]\n",
      " [   1    0    4  998    0    2    0    2    3    0]\n",
      " [   0    0    0    0  977    0    2    0    0    3]\n",
      " [   0    0    1    7    0  880    3    1    0    0]\n",
      " [   4    2    0    0    1    1  948    0    2    0]\n",
      " [   0    1    2    0    1    0    0 1022    1    1]\n",
      " [   2    0    1    1    0    0    0    2  967    1]\n",
      " [   0    0    0    0    6    1    0    4    1  997]], shape=(10, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, val_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Get predicitions and confusion matrix\n",
    "pred_test = model.predict(x_test)\n",
    "print(tf.math.confusion_matrix(y_test.argmax(axis=1), pred_test.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('mnist_lenet5.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
